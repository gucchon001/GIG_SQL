# common_create_datasets.py

import os
import pandas as pd
import mysql.connector
import time
from core.config.config_loader import load_config
try:
    # æ–°æ§‹é€ ã®ãƒ­ã‚°ç®¡ç†ã‚’å„ªå…ˆä½¿ç”¨
    from src.core.logging.logger import get_logger
    LOGGER = get_logger('datasets')
except ImportError:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæ—§æ§‹é€ 
    from core.config.my_logging import setup_department_logger
    LOGGER = setup_department_logger('datasets', app_type='datasets')
from .subcode_loader import (
    load_sql_file_list_from_spreadsheet, 
    get_data_types, 
    apply_data_types_to_df, 
    execute_sql_query_with_conditions,
    csvfile_export,
    parquetfile_export,
    export_to_spreadsheet,
    setup_test_environment
)
from ..utils.db_utils import get_connection
try:
    # æ–°æ§‹é€ ã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’å„ªå…ˆä½¿ç”¨
    from src.utils.data_processing import format_dates
except ImportError:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæ—§æ§‹é€ 
    def format_dates(df, data_types):
        """æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡¦ç†ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…"""
        for column, data_type in data_types.items():
            if column in df.columns:
                try:
                    if data_type == 'date':
                        df[column] = pd.to_datetime(df[column], errors='coerce').dt.strftime('%Y/%m/%d')
                    elif data_type == 'datetime':
                        df[column] = pd.to_datetime(df[column], errors='coerce').dt.strftime('%Y/%m/%d %H:%M:%S')
                except Exception as e:
                    LOGGER.error(f"æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e} (åˆ—: {column})")
                    df[column] = pd.NaT
        return df

def main(sheet_name, execution_column, config_file, selected_table=None):
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    ssh_config, db_config, local_port, additional_config = load_config(config_file)

    # ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
    # LOGGER ã¯ä¸Šè¨˜ã®tryãƒ–ãƒ­ãƒƒã‚¯ã§è¨­å®šæ¸ˆã¿

    # SQLãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã®å–å¾—
    spreadsheet_id = additional_config['spreadsheet_id']
    json_keyfile_path = additional_config['json_keyfile_path']

    # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
    LOGGER.info(f"å‡¦ç†å¯¾è±¡ã‚·ãƒ¼ãƒˆ: {sheet_name}")
    LOGGER.info(f"å®Ÿè¡Œåˆ—: {execution_column}")

    sql_files_list = load_sql_file_list_from_spreadsheet(
        spreadsheet_id, 
        sheet_name, 
        json_keyfile_path, 
        execution_column=execution_column
    )
    # sql_files_list ã®å†…å®¹ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
    LOGGER.debug("sql_files_list ã®å†…å®¹:")
    for i, entry in enumerate(sql_files_list):
        LOGGER.debug(f"ã‚¨ãƒ³ãƒˆãƒªãƒ¼ {i}:")
        LOGGER.debug(f"  sql_file_name: {entry[0]}")
        LOGGER.debug(f"  csv_file_name: {entry[1]}")
        LOGGER.debug(f"  period_condition: {entry[2]}")
        LOGGER.debug(f"  period_criteria: {entry[3]}")
        LOGGER.debug(f"  deletion_exclusion: {entry[4]}")
        LOGGER.debug(f"  category: {entry[5]}")
        LOGGER.debug(f"  main_table_name: {entry[6]}")
        if len(entry) > 7:
            LOGGER.debug(f"  ãã®ä»–ã®è¦ç´ : {entry[7:]}")

    def save_to_parquet(df, output_path):
        try:
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆã™ã‚‹å‰ã«ã€ç¾åœ¨ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åˆ—ã¨ã—ã¦ä¿å­˜
            if df.index.name is None:
                df['original_index'] = df.index
            else:
                df[df.index.name] = df.index

            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆã—ã€é™é †ã§ã‚½ãƒ¼ãƒˆ
            df_sorted = df.reset_index(drop=True).sort_values('original_index', ascending=False)
            
            # 'original_index'åˆ—ã‚’å‰Šé™¤
            df_sorted = df_sorted.drop('original_index', axis=1)
            
            df_sorted.to_parquet(output_path, engine='pyarrow', index=False)
            LOGGER.info(f"ãƒ‡ãƒ¼ã‚¿ã‚’Parquetå½¢å¼ã§é™é †ã§ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
        except Exception as e:
            LOGGER.error(f"Parquetä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    conn = get_connection(config_file)
    if conn:
        processed_count = 0
        total_count = len([e for e in sql_files_list if not selected_table or e[11] == selected_table])
        
        for entry in sql_files_list:
            sql_file_name, csv_file_name, period_condition, period_criteria, save_path_id, output_to_spreadsheet, deletion_exclusion, paste_format, test_execution, category, main_table_name, csv_file_name_column, sheet_name_record = entry

            # ãƒ†ãƒ¼ãƒ–ãƒ«åãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€CSVãƒ•ã‚¡ã‚¤ãƒ«å‘¼ç§°ã§åˆ¤å®š
            if selected_table and csv_file_name_column != selected_table:
                LOGGER.info(f"ã‚¹ã‚­ãƒƒãƒ—: {csv_file_name_column} ã¯é¸æŠã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«å‘¼ç§°ï¼ˆ{selected_table}ï¼‰ã«å¯¾å¿œã—ã¾ã›ã‚“ã€‚")
                continue
                
            processed_count += 1
            LOGGER.info(f"ğŸ“Š å‡¦ç†ä¸­ ({processed_count}/{total_count}): {main_table_name} ã‚’é–‹å§‹ã—ã¾ã™")

            # ãƒ†ã‚¹ãƒˆç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
            try:
                save_path_id, csv_file_name = setup_test_environment(
                    test_execution,
                    output_to_spreadsheet,
                    save_path_id,
                    csv_file_name,
                    additional_config['spreadsheet_id'],
                    additional_config['json_keyfile_path']
                )
                LOGGER.info(f"ãƒ†ã‚¹ãƒˆç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸ: save_path_id={save_path_id}, csv_file_name={csv_file_name}")
            except Exception as e:
                LOGGER.error(f"ãƒ†ã‚¹ãƒˆç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                continue

            LOGGER.debug(f"å‡¦ç†ä¸­ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼:")
            LOGGER.debug(f"  sql_file_name: {sql_file_name}")
            LOGGER.debug(f"  csv_file_name: {csv_file_name}")
            LOGGER.debug(f"  period_condition: {period_condition}")
            LOGGER.debug(f"  period_criteria: {period_criteria}")
            LOGGER.debug(f"  save_path_id: {save_path_id}")
            LOGGER.debug(f"  output_to_spreadsheet: {output_to_spreadsheet}")
            LOGGER.debug(f"  deletion_exclusion: {deletion_exclusion}")
            LOGGER.debug(f"  category: {category}")
            LOGGER.debug(f"  main_table_name: {main_table_name}")

            display_name = csv_file_name
            try:
                LOGGER.debug(f"mainå‡¦ç† - deletion_exclusion: {deletion_exclusion}")
                sql_query = execute_sql_query_with_conditions(
                    sql_file_name,
                    additional_config,
                    period_condition,
                    period_criteria,
                    deletion_exclusion,
                    category,
                    main_table_name
                )
                if sql_query:
                    # SQLæœ¬æ–‡ã®ãƒ­ã‚°å‡ºåŠ›ã¯æŠ‘åˆ¶
                    try:
                        LOGGER.info(f"å®Ÿè¡ŒSQL - ãƒ•ã‚¡ã‚¤ãƒ«å: {sql_file_name}ï¼ˆæœ¬æ–‡éè¡¨ç¤º, é•·ã•: {len(sql_query)} æ–‡å­—ï¼‰")
                    except Exception:
                        LOGGER.info(f"å®Ÿè¡ŒSQL - ãƒ•ã‚¡ã‚¤ãƒ«å: {sql_file_name}ï¼ˆæœ¬æ–‡éè¡¨ç¤ºï¼‰")
                    
                    # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®è¨­å®šã«åŸºã¥ã„ã¦å‡ºåŠ›å‡¦ç†ã‚’åˆ†å²
                    if output_to_spreadsheet == 'CSV':
                        # CSVå‡ºåŠ›
                        if save_path_id and save_path_id.strip():
                            csv_file_path = os.path.join(save_path_id, csv_file_name)
                        else:
                            csv_file_path = os.path.join(additional_config['csv_base_path'], csv_file_name)
                        
                        try:
                            csvfile_export(
                                conn,
                                sql_query,
                                csv_file_path,
                                main_table_name,
                                category,
                                additional_config['json_keyfile_path'],
                                additional_config['spreadsheet_id'],
                                csv_file_name,
                                csv_file_name_column,
                                sheet_name_record,
                                additional_config.get('chunk_size'),
                                additional_config.get('delay')
                            )
                            LOGGER.info(f"âœ… å®Œäº† ({processed_count}/{total_count}): {main_table_name} -> {csv_file_path}")
                        except Exception as e:
                            LOGGER.error(f"âŒ ã‚¨ãƒ©ãƒ¼ ({processed_count}/{total_count}): {main_table_name} - {e}")
                            
                    elif output_to_spreadsheet == 'ã‚¹ãƒ—ã‚·':
                        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆå‡ºåŠ›
                        if not csv_file_name:
                            csv_file_name = csv_file_name_column
                        try:
                            export_to_spreadsheet(
                                conn,
                                sql_query,
                                save_path_id,
                                csv_file_name,
                                additional_config['json_keyfile_path'],
                                paste_format,
                                sheet_name_record,
                                csv_file_name_column,
                                main_table_name,
                                category,
                                additional_config.get('chunk_size'),
                                additional_config.get('delay')
                            )
                            LOGGER.info(f"âœ… å®Œäº† ({processed_count}/{total_count}): {main_table_name} -> ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ: {csv_file_name}")
                        except Exception as e:
                            LOGGER.error(f"âŒ ã‚¨ãƒ©ãƒ¼ ({processed_count}/{total_count}): {main_table_name} - {e}")
                            
                    elif output_to_spreadsheet == 'parquet':
                        # Parquetå‡ºåŠ›
                        # SQLãƒ•ã‚¡ã‚¤ãƒ«åãƒ™ãƒ¼ã‚¹ã§parquetãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆStreamlitã¨ã®æ•´åˆæ€§ã®ãŸã‚ï¼‰
                        base_name = os.path.splitext(sql_file_name)[0]
                        parquet_filename = f"{base_name}.parquet"
                        
                        if save_path_id and save_path_id.strip():
                            parquet_file_path = os.path.join(save_path_id, parquet_filename)
                        else:
                            parquet_file_path = os.path.join(additional_config['csv_base_path'], parquet_filename)
                        
                        try:
                            parquetfile_export(
                                conn,
                                sql_query,
                                parquet_file_path,
                                main_table_name,
                                category,
                                additional_config['json_keyfile_path'],
                                additional_config['spreadsheet_id'],
                                csv_file_name,
                                csv_file_name_column,
                                sheet_name_record,
                                additional_config.get('chunk_size'),
                                additional_config.get('delay')
                            )
                            LOGGER.info(f"âœ… å®Œäº† ({processed_count}/{total_count}): {main_table_name} -> {parquet_file_path}")
                            LOGGER.info(f"ğŸ‰ ãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº†: {main_table_name} - æ­£å¸¸ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
                        except Exception as e:
                            LOGGER.error(f"âŒ ã‚¨ãƒ©ãƒ¼ ({processed_count}/{total_count}): {main_table_name} - {e}")
                    else:
                        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‡¦ç†ï¼ˆå¾“æ¥ã®Parquetå‡ºåŠ›ï¼‰
                        # MySQLæ¥ç¶šã‚¨ãƒ©ãƒ¼ã®å¯¾å¿œ: å†æ¥ç¶šæ©Ÿèƒ½ä»˜ãã§SQLå®Ÿè¡Œ
                        max_sql_retries = 3
                        sql_retry_count = 0
                        df = None
                        
                        while sql_retry_count < max_sql_retries:
                            try:
                                # æ¥ç¶šçŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯
                                conn.ping(reconnect=True)
                                df = pd.read_sql_query(sql_query, conn)
                                LOGGER.info(f"SQLã‚¯ã‚¨ãƒªå®Ÿè¡ŒæˆåŠŸ: {sql_file_name}")
                                break
                            except mysql.connector.Error as sql_err:
                                sql_retry_count += 1
                                LOGGER.warning(f"SQLå®Ÿè¡Œã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {sql_retry_count}/{max_sql_retries}): {sql_err}")
                                if sql_retry_count >= max_sql_retries:
                                    LOGGER.error(f"SQLå®Ÿè¡Œã®æœ€å¤§è©¦è¡Œå›æ•°ã«é”ã—ã¾ã—ãŸ: {sql_file_name}")
                                    raise sql_err
                                LOGGER.info("5ç§’å¾Œã«SQLå®Ÿè¡Œã‚’å†è©¦è¡Œã—ã¾ã™...")
                                time.sleep(5)
                            except Exception as sql_err:
                                sql_retry_count += 1
                                LOGGER.warning(f"äºˆæœŸã—ãªã„SQLå®Ÿè¡Œã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {sql_retry_count}/{max_sql_retries}): {sql_err}")
                                if sql_retry_count >= max_sql_retries:
                                    LOGGER.error(f"SQLå®Ÿè¡Œã®æœ€å¤§è©¦è¡Œå›æ•°ã«é”ã—ã¾ã—ãŸ: {sql_file_name}")
                                    raise sql_err
                                LOGGER.info("5ç§’å¾Œã«SQLå®Ÿè¡Œã‚’å†è©¦è¡Œã—ã¾ã™...")
                                time.sleep(5)
                        
                        if df is None:
                            LOGGER.error(f"SQLã‚¯ã‚¨ãƒªã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ: {sql_file_name}")
                            continue
                        if not df.empty:
                            LOGGER.info(f"{display_name}ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
                            
                            # ãƒ‡ãƒ¼ã‚¿å‹ã‚’æŒ‡å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‡¦ç†ï¼‰
                            data_types = {}
                            LOGGER.info("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‡ãƒ¼ã‚¿å‹å‡¦ç†ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                            
                            # æ—¥ä»˜ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’çµ±ä¸€
                            df = format_dates(df, data_types)
                            
                            # ãƒ‡ãƒ¼ã‚¿å‹ã‚’é©ç”¨
                            try:
                                df = apply_data_types_to_df(df, data_types, LOGGER)
                            except Exception as e:
                                LOGGER.error(f"å‹å¤‰æ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                                continue
                            
                            # ä¿å­˜å…ˆã®æ±ºå®š
                            if save_path_id and save_path_id.strip():
                                output_dir = save_path_id
                            else:
                                output_dir = additional_config['csv_base_path']
                            
                            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
                            if not os.path.exists(output_dir):
                                try:
                                    os.makedirs(output_dir)
                                    LOGGER.info(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ: {output_dir}")
                                except Exception as e:
                                    LOGGER.error(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                            
                            # SQLãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ‹¡å¼µå­ã‚’.parquetã«å¤‰æ›´
                            base_name = os.path.splitext(sql_file_name)[0]
                            output_file_path = os.path.join(output_dir, f"{base_name}.parquet")
                            save_to_parquet(df, output_file_path)
                        else:
                            LOGGER.error(f"{display_name}ã®ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
                else:
                    LOGGER.error(f"{display_name}ã®SQLã‚¯ã‚¨ãƒªã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            except Exception as e:
                LOGGER.error(f"SQLã‚¯ã‚¨ãƒªã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                continue
        conn.close()
        LOGGER.info("=" * 50)
        LOGGER.info("ğŸ‰ å…¨ã¦ã®å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ - SUCCESS")
        LOGGER.info("=" * 50)
    else:
        LOGGER.error("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
